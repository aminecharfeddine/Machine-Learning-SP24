{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You: Amine Charfeddine\n",
    "\n",
    "Teammate: Delil Dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the eleventh lab. Deep Learning takes the already complex topic of Neural Networks and turns it up a notch. Several notches, in fact. It's hard to find exercises small enough to fit in a single assignment, let alone a *set* of exercises for all of these topics.\n",
    "\n",
    "So this week the assignment is particularly small, with only 15 points, and should not take you as long as usual to complete. What you should do if you are interested in building Deep Learning experience instead is take one of the Bonus Questions and solve it yourself. We will support fully any question on any of the topics.\n",
    "\n",
    "Willing to learn but unsure on the topic? Go for the **Transfer Learning** tutorial, it's the shortest and one of the most marketable skills. Basically you download a pre-trained network (on a huge dataset), cut the last (decision) layer(s), add your own (so you decide based on the last feature space), then train _only your new layer(s)_ on your specific task, which is fast and easy. While re-using the larger body that was pre-trained by someone else, likely with a larger budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 10 points (out of 15 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. History and training strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Mention 3 reasons why DL did not happen 30 years ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Insufficient Computational Power: Hardware 30 years ago could not handle the intensive computations required by deep learning models.\n",
    "\n",
    "2. Limited Data Availability: There wasn't enough large-scale data available for training deep learning models effectively.\n",
    "\n",
    "3. Algorithmic and Theoretical Advancements: Key algorithms and techniques for deep learning were not yet developed or well-understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[1pt]** Explain how to train a neural network using supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by collecting and preprocessing a labeled dataset, ensuring the input data is normalized. This includes steps like data cleaning and handling missing values. Then, we define the neural network architecture (input, hidden, output layers) and initialize the weights. During training, we perform a forward pass by feeding inputs through the network to get predictions. Then, we calculate the loss between these predictions and the true labels using an appropriate loss function. Afterward, we use backpropagation to compute gradients of the loss with respect to each weight, and update the weights using an optimization algorithm like SGD. We repeat this process for multiple epochs until the performance of the model stabilizes. Also, we validate the model on a separate validation set to monitor its performance and prevent overfitting. Finally, we can adjust hyperparameters as needed to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.3 **[1pt]** What is overfiting? How to avoid it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Overfitting occurs when a machine learning model learns the training data too well, capturing noise and details that do not generalize to new, unseen data. This results in poor performance on validation or test sets. To avoid overfitting, several techniques can be employed: using cross-validation, data augmentation, simplifying the model architecture, and increasing the amount of training data. Data augmentation helps by artificially generating more training data from existing samples, thus increasing data diversity. Cross-validation ensures the model performs consistently across different subsets of data, and simplifying the model reduces its complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[2pt]** Calculate the dimension of the feature space of the third layer of LeNet-5 (16 filters, slide 28). Explain your reasoning.\n",
    "\n",
    "- Remember it uses Valid Convolution for padding.\n",
    "- The filter size is really $(5\\times5\\times6)$ since it takes all channels at a time.\n",
    "- The number of Filters is the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, the input layer has dimensions 32x32. It has 1 channel, representing the grayscale image (an \"A\" in our case).\n",
    "<br>\n",
    "\n",
    "Then, the first convolutional layer (named C1) has 6 filters of size 5x5 each, applied with a stride of 1 (written in the slide : \"conv filters were 5x5, applied at stride 1\"). <br> The output dimensions can be calculated using the formula : <br>\n",
    "Output size = $\\lfloor \\frac{(N - F)}{S} \\rfloor + 1$ where N is the input size, F the filter size and S the stride. \n",
    "<br> Therefore, we have : output size = $\\lfloor \\frac {(32 - 5)}{1} \\rfloor + 1 = 28 $ <br>\n",
    "So the output dimensions are 28x28x6 (also written 6@28x28).\n",
    "\n",
    "Now, S2. The subsampling layers were \"2x2 applied at stride 2\". <br>\n",
    "Therefore : output size = $\\lfloor \\frac{28}{2} \\rfloor = 14 $ <br>\n",
    "So the output dimensions are 14x14x6. <br>\n",
    "\n",
    "Finally, C3 has 16 filters of size 5x5. The input, is the output of S2, meaning 14x14x6. <br> Therefore, using the same formula than in C1, we have : output size = $\\lfloor \\frac {(14 - 5)}{1} \\rfloor + 1 = 10 $ <br>\n",
    "So the output dimensions of the third layer C3 are 10x10x16 (in the slide it's written as 16@10x10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[2pt]** Explain in English the results of the Microsoft Tay Twitter chatbot experiment. Propose a safer alternative experiment protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft's Tay Twitter chatbot was designed to learn from conversations with users. However, within hours, Tay began posting offensive and inappropriate tweets because users taught it harmful language. The bot lacked filters to block such content, leading to a quick backlash. As a result, Microsoft had to shut Tay down within 24 hours. <br>\n",
    "\n",
    "To make chatbot experiments safer, several steps can be taken. First, test the bot in a controlled environment to identify issues early. Second, use filters and human moderators to block harmful content. Third, train the bot with carefully selected data. Another idea is to release the bot gradually to a small group of users for close monitoring before releasing it to the public. To maintain safety, they could also allow users to report bad behavior and keep humans involved to review conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** GANs are amazing tools and a great topic, but they are complex enough that implementing a decent example would require a lab by itself. So here is a [great tutorial](https://colab.research.google.com/github/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb), if you choose to play with it share your progress on Moodle and we'll support you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to only do **one** Bonus Question in the entire course, and was interested in taking a job using Deep Learning after the university, I would do this one here. It is too much work to complete to _require_ everyone to do it, but is probably the most valuable exercise in this whole assignment if you wish to do it.\n",
    "\n",
    "Transfer Learning is easily the most useful and powerful technique to know when you first get a job that expects you to apply Deep Learning -- granted, IF you know how NNs work, as required for this course. It allows you to simply download enormous networks that have been trained on supercomputers using unbelievably large datasets, then specialize them your specific problem and use their results for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Follow [this tutorial](https://keras.io/guides/transfer_learning/) on Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[5pt]** Run the simple Transformer tutorial below to train a model on movie reviews. Explain what parts of the Transformer are involved in the following lines, and what do they do: 33, 37, 66, 106, 109 (include each parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 92s 114ms/step - loss: 0.3793 - accuracy: 0.8224 - val_loss: 0.2911 - val_accuracy: 0.8734\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 0.1972 - accuracy: 0.9247 - val_loss: 0.3259 - val_accuracy: 0.8709\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Text classification with Transformer\n",
    "Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n",
    "Date created: 2020/05/10\n",
    "Last modified: 2020/05/10\n",
    "Description: Implement a Transformer block as a Keras layer\n",
    "and use it for text classification.\n",
    "Accelerator: GPU\n",
    "https://github.com/keras-team/keras-io/blob/master/examples/nlp/text_classification_with_transformer.py\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# The IMDB Large Movie Review Dataset\n",
    "# https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "\"\"\"\n",
    "## Implement a Transformer block as a layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                             key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token\n",
    "index (positions).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
    "                                          output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen,\n",
    "                                        output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Download and prepare dataset\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "dataset = imdb.load_data(num_words=vocab_size)\n",
    "(x_train, y_train), (x_val, y_val) = dataset\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "\"\"\"\n",
    "## Create classifier model using transformer layer\n",
    "\n",
    "Transformer layer outputs one vector for each time\n",
    "step of our input sequence. Here, we take the mean\n",
    "across all time steps and use a feed forward network\n",
    "on top of it to classify text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Train and Evaluate\n",
    "\"\"\"\n",
    "\n",
    "batch_size=32\n",
    "epochs=2\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Line 33**: This line initializes a multi-head attention layer using layers.MultiHeadAttention, a key part of the Transformer architecture. The parameter num_heads specifies the number of attention heads. Multiple heads allow the model to focus on different parts of the input sequence at the same time, capturing various features and relationships within the data. As seen in the lecture, this is a key advantage of using transformers compared to regular RNNs, the latter processing data sequentially rather than in parallel. The parameter key_dim=embed_dim sets the dimension of the key, query, and value vectors. This parallel processing helps the model understand context better and handle complex sequences more effectively.\n",
    "\n",
    "- **Line 37**: A dense layer within a feed forward network is created using layers.Dense(ff_dim, activation=\"relu\"). The ff_dim parameter sets the number of units (neurons) in this dense layer, determining the capacity of the feed forward network to learn from the input data. The activation=\"relu\" (Rectified Linear Unit) introduces non-linearity, helping the network learn complex patterns. The feed forward network applies this dense layer to each position in the sequence independently. This transforms the attention output into more useful representations for the next layers, improving the model's prediction accuracy. As discussed in the lecture, this feed forward network is crucial for adding non-linear transformations and enhancing the model's ability to capture complex dependencies in the data.\n",
    "\n",
    "- **Line 66**: This line sets up an embedding layer for tokens with self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim). The input_dim=vocab_size parameter specifies the size of the input vocabulary, indicating how many unique tokens (words) the embedding layer can handle. The output_dim=embed_dim sets the dimension of the dense embedding vectors. Each token in the input sequence will be represented as a dense vector of this fixed size, which is the first step in processing the text data for further transformations by the model. As mentioned in the lecture, embedding layers are essential for converting sparse token indices into dense vector spaces.\n",
    "\n",
    "- **Line 106**: Line 106: This line defines the input layer of the model with inputs = layers.Input(shape=(maxlen,)). The layers.Input function sets the shape and type of input data that the model will receive. The parameter shape=(maxlen,) indicates that the input will be a sequence of integers (word indices) with a maximum length of maxlen. It ensures that all input sequences are of the same length and properly structured before being processed by the subsequent Transformer layers. As discussed in the lecture, having a defined input shape is crucial for maintaining consistency and \n",
    "compatibility in the model's architecture.\n",
    "\n",
    "- **Line 109**: This line creates an instance of the \"TransformerBlock\" class and assigns it to the variable transformer_block with transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim). The \"TransformerBlock\" combines multi-head attention and a feed forward network. This combination forms the core of the Transformer model. It processes the embedded input sequence, capturing complex patterns, essential for accurate text classification. The parameter \"embed_dim\" sets the size of the embeddings, used in both the multi-head attention mechanism and the feed forward network. The parameter \"num_heads\" specifies the number of attention heads. Finally, the parameter \"ff_dim\" defines the size of the hidden layer in the feed forward network, determining its capacity to learn from the data (see explanation from **line 37**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[3pt]** Use the trained model to generate a review of 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that your `model` is, after all, still just a neural network.\n",
    "- Think what input you need to pass at each time step, you already did sequential modeling with RNNs last week.\n",
    "- Think explicitly about the conversions between text and embedded vectors, both for inputs and outputs.\n",
    "- For your reference, here is some code to read a (decoded) review from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# Source: Tensorflow IMDB dataset documentation\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index\n",
    "\n",
    "# Use the default parameters to keras.datasets.imdb.load_data\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "# Retrieve the training sequences.\n",
    "(x_train, _), _ = keras.datasets.imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")\n",
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 155s 975ms/step - loss: 7.4535\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 148s 941ms/step - loss: 6.5776\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 148s 945ms/step - loss: 6.1074\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 148s 942ms/step - loss: 5.8633\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 149s 949ms/step - loss: 5.6742\n",
      "This movie bushwhackers macbeth overlooking stoolie dracula jewels sherlock 2004 cane disappoints chandler stepping prosthetic williamson lusts prudish sales negative park documentary britney bristol sexist symbols consideration deranged hanzo jam snobbery critical finally twenties lynne scout feels colin deepest goldeneye expression apocalypse drain hadn't march 26 'oh subtext throats heaving fencing figured hawke autumn team degenerate montana binding derive sept cassandra sham sheryl sacrificial voight bemused unintentional peep unraveling comparisons dola impaired veers beset personal predator montages fosters terrorizing differences italian standout annoyed packaged brighter posted einstein's youths fx tunnel composure shy screaming institute biko's enforced cyborgs husband statistics facing forget headaches\n"
     ]
    }
   ],
   "source": [
    "# I found this question a bit hard because I felt like the transformer part was mostly focused on sentiment analysis \n",
    "# and I did not figure out how to use the model to generate a review, so I searched on the web to find an alternative way.\n",
    "# As such, I wrote this code by copying many parts of code I found on the web and tailoring it to our example.\n",
    "# In the code, I used an LSTM-based language model to generate a review. I prepared the IMDB dataset, built and trained \n",
    "# an LSTM model, and then implemented a function to generate text using this model. The text generation involves \n",
    "# predicting the next word iteratively and appending it to the seed text. The output is a review of 100 words but it does \n",
    "# not really make sense, maybe this could be improved by increasing the training data, adjusting the model's complexity, \n",
    "# or tuning the hyperparameters (but when I tried it takes a lot of time to run).\n",
    "\n",
    "vocab_size = 20000  # we only consider the top 20000 words \n",
    "maxlen = 100  # we only consider the first 100 words of each movie review\n",
    "\n",
    "(x_train, _), _ = imdb.load_data(num_words=vocab_size) #we load the IMDB dataset\n",
    "\n",
    "x_train = x_train[:10000]  # we reduce the training set to the first 10000 examples\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen) # we pad sequences to the same length (100 words)\n",
    "\n",
    "embed_dim = 16  # embedding size for each token (word vectors)\n",
    "lstm_units = 64  # number of LSTM units in our model\n",
    "\n",
    "# We now build the language model\n",
    "inputs = layers.Input(shape=(maxlen,)) # input layer\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs) # we add an embedding layer that converts word indices to word vectors\n",
    "x = layers.LSTM(lstm_units, return_sequences=True)(x) # we add an LSTM layer that processes the sequences\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x) # we add a dense output layer with softmax activation to predict the next word\n",
    "\n",
    "language_model = keras.Model(inputs, outputs)\n",
    "\n",
    "language_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') #we compile the model\n",
    "\n",
    "y_train = np.expand_dims(x_train, axis=-1) # we prepare the target data\n",
    "\n",
    "# We now train the language model\n",
    "language_model.fit(x_train, y_train, batch_size=64, epochs=5)  # 5 epochs for better training\n",
    "\n",
    "def generate_review(model, seed_text, tokenizer, maxlen, review_length=100, temperature=1.0):\n",
    "    for _ in range(review_length):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]  # we tokenize the input text (convert into squences of word indices)\n",
    "        token_list = sequence.pad_sequences([token_list], maxlen=maxlen, padding='pre') # we ensure the sequence it has the same length (maxlen) \n",
    "\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0, -1, :]  # we predict the next word probabilities\n",
    "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs)) # normalization\n",
    "        predicted_word_index = np.random.choice(range(vocab_size), p=predicted_probs) # randomly selects an index, but the word with higher probability is more likely to be chosen\n",
    "\n",
    "        if predicted_word_index >= vocab_size: # case where perdicted word index is out of range\n",
    "            predicted_word_index = 2  # index for '<unknown>'\n",
    "\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '<unknown>')\n",
    "        seed_text += \" \" + predicted_word  # we append the predicted word to the seed text\n",
    "        if predicted_word == '<end>':  # we break if the model predicts the end of the review\n",
    "            break\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "# We repare the tokenizer with the word index\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {index: word for word, index in word_index.items()}  # we reverse index to decode sequences back to words\n",
    "# We define special tokens (markers that help guide the model)\n",
    "index_word[0] = '<pad>' \n",
    "index_word[1] = '<start>'\n",
    "index_word[2] = '<unknown>'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer() # we initialize the tokenizer \n",
    "tokenizer.word_index = word_index\n",
    "tokenizer.index_word = index_word\n",
    "\n",
    "seed_text = \"This movie\" # This is the start of the review\n",
    "\n",
    "generated_review = generate_review(language_model, seed_text, tokenizer, maxlen) # We generate the review\n",
    "print(generated_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS [ZERO pt]: Follow this complete tutorial on Transformers.**  \n",
    "https://www.tensorflow.org/text/tutorials/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** You now know the basis for time series prediction using recurrent networks. Why don't you try your hand at predicting the evolution of the current COVID-19 situation? Specifically look at the Reproduction number, which is the base for the exponential growth of the infection. You can find the main data from JHU CSSE [here](https://github.com/CSSEGISandData/COVID-19), then the data for Switzerland [here](https://github.com/openZH/covid_19) (specifically Fribourg [here](https://github.com/openZH/covid_19/blob/master/fallzahlen_kanton_total_csv_v2/COVID19_Fallzahlen_Kanton_FR_total.csv)), some work from ETHZ [here](https://bsse.ethz.ch/cevo/research/sars-cov-2/real-time-monitoring-in-switzerland.html), and an example for advanced visualization [here](https://opensource.com/article/20/4/python-data-covid-19). Feel free to share your conclusions and opinions on it on the forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You now know more about Transformers networks using Keras: this allows you to tackle several Natural Language Processing tasks, which are highly marketable at this time.\n",
    "- You should now have a deeper understanding of convolutions, especially on how things that appear small and easy (such as padding and striding) can lead to quite complex changes of behavior. For example, can we apply \"same\" padding with an filter of an even shape (e.g. 4 x 4, 6 x 6 etc.)? Would it be possible to pad the input such that, using a stride > 1, we get a matrix with the same shape as the input? This reasoning is important because these \"sizes\" in the network are hyperparameters, which means that you are responsible to set them correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-KERNEL",
   "language": "python",
   "name": "ml-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
