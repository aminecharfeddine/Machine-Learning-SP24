{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You:\n",
    "\n",
    "Teammate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the twelfth lab. It's finally time to look closely at the foundations of Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 14 points (out of 21 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Explain the Reinforcement Learning paradigm in English. Use the words Environment, Agent, Action and Feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[1pt]** Explain the equation for (pseudo-)Regret in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[1pt]** Explain in English the importance of _exploration vs. exploitation_ in MAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 **[1pt]** Explain what would happen if you had a discount factor $\\gamma \\gt 1$ in the Ice Maze example from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 **[2pt]** The Bellman Equation is at the heart of the classical Reinforcement Learning framework. Write it below in Latex, then explain each of the terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The [OpenAI Gym](https://www.gymlibrary.dev/) maintains a broad set of Reinforcement Learning benchmarks. It is ready to import on Colab, or you can add to your local installation the environments we will use for this assignment using `pipenv install gym[toy_text,classic_control]`.\n",
    "- You can find the Ice Maze (named Frozen Lake) [here](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- The Classic Control games are also relatively easy to solve, and much nicer to render. Feel free to give it a try. You can save a video with `env = gym.wrappers.Monitor(env, 'video', force = True)` right after the `gym.make()` call. More on this topic [here](https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/).\n",
    "- To get you started, here is a working example of a random policy on the framework, which I took from their front page and customized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[2pt]** Write a Python function `choose_action` that takes as arguments `Q`, `state`, `epsilon`, `actions` and returns an action chosen according to Q-values using an epsilon-greedy policy.\n",
    "\n",
    "- `Q` is a dictionary that has states as keys. Each value is a numpy array with the Q-values corresponding to each action from the state used as key.\n",
    "- You cannot initialize `Q` with all possible states, because they can be infinite or unreachable. Using `Q[state]` with missing key `state` will throw an error; use instead `Q.get(state)` then check if its return `is None`.\n",
    "- `state` can be simply a number.\n",
    "- `actions` is a list of possible actions between which to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: here is some code I used for testing `choose_action()` and expected outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {1:[0,0,1], 2:[0,1,0]}\n",
    "for _ in range(50):\n",
    "    print(choose_action(Q, 1, 0.0, []), end='') #=> 2\n",
    "print()\n",
    "for _ in range(50):\n",
    "    print(choose_action(Q, 2, 0.0, []), end='') #=> 1\n",
    "print()\n",
    "for _ in range(50):\n",
    "    print(choose_action(Q, 2, 1.0, range(3)), end='') #=> random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[2pt]** Write a Python function `update_Q` that takes as arguments `Q`, `state`, `action`, `next_state`, `reward` and `num_actions`, and updates the `Q` dictionary according to Q-Learning. \n",
    "\n",
    "- Here you will need to initialize `Q[state]` if previously unexplored.\n",
    "- The future expected reward is the highest Q value from the next state -- or zero if unexplored. That's what the last argument is for.\n",
    "- I'll leave the testing to you here. You may feel like just running the next question for testing (called _integration testing_ ), but testing it in isolation ( _unit testing_ ) is a much more controlled verification which should not be skipped (i.e. _easier_ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 **[1pt]** Run the code below to successfully use Q-Learning to solve the OpenAI Gym `FrozenLake` environment.\n",
    "\n",
    "- The code below is already correct, it just needs the two functions from above.\n",
    "- If you got both previous answers right, yes this is a free extra point for you :) just run it to produce the outputs.\n",
    "- If this does not run correctly, something is likely wrong with your implementation above. Go back and test a bit more.\n",
    "- The parameters should work as they are, but feel free to play with them and make sure by the last epochs the environment is solved more often than not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and settings\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "num_episodes = 1000\n",
    "max_nsteps = 30\n",
    "gamma = 0.9\n",
    "alpha = 0.6\n",
    "epsilon = 0.3\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "Q = {} # hashing states to per-action reward arrays\n",
    "\n",
    "# Loop for each episode\n",
    "for ith_episode in range(num_episodes):\n",
    "\n",
    "    # Reset the environment (and obtain first observation)\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    # For each timestep\n",
    "    for t in range(max_nsteps):\n",
    "        \n",
    "        # Choose action according to Q-values using epsilon-greedy policy\n",
    "        # THIS SHOULD USE YOUR IMPLEMENTATION\n",
    "        action = choose_action(Q, state, epsilon, range(num_actions))\n",
    "        \n",
    "        # Execute action: get reward, move env to next state\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        # Add negative reward for falling in hole\n",
    "        if done and reward == 0: reward = -1\n",
    "\n",
    "        # Some useful printing to verify progression - (W)in or (l)ose\n",
    "        if reward == -1: print('l', end='', flush=True)\n",
    "        if reward == 1: print('W', end='', flush=True)\n",
    "\n",
    "        # Accumulate reward. Note this environment only rewards on termination though.\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update Q function for current state and action\n",
    "        # THIS SHOULD USE YOUR IMPLEMENTATION\n",
    "        update_Q(Q, state, action, next_state, reward, num_actions, alpha, gamma)\n",
    "        \n",
    "        # Update internal state\n",
    "        state = next_state\n",
    "        \n",
    "        # Terminate if episode ended\n",
    "        if done: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 **[1pt]** Write a Python code snippet that satisfies the following requirements: (i) runs a fully-greedy policy, (ii) using the Q-values learned in question 2.3, (iii) renders the environment in each step, and (iv) write less than 10 lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The point of this question is for you to go back to 2.3, study it and understand it. If you get how it works, this snippet will take but a second.\n",
    "- For the subpoint (iii) you should look at the method `render()` of the OpenAI environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 **[1pt]** Modify the code from question 2.3 to record the cumulative reward and number of time-steps for each episode. Then plot them with two line plots using Seaborn.\n",
    "\n",
    "- Think carefully: you need to augment the code above, by first initializing a proper data structure, then recording the statistics at the end of each episode. Finally plot them below.\n",
    "- If the plot looks uniform while you expect improvement, try re-running it a few times. Remember that the exploration capabilities of this methods are pretty limited, so short runs (such as this) depend significantly on the initialization conditions (i.e. luck)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN basically implements Q-Learning but uses a (deep) neural network to learn the rewards (with a few tricks). The main advantage versus the dictionary-based approach above is *generalization*: the network can output an expected reward for **all actions**, even those that were not yet explored.\n",
    "- This code takes a while to execute. You may want to use [TPUs with Colab](https://www.tensorflow.org/guide/tpu)\n",
    "  - Though if the `env.render()` does not work in that case, just comment it out\n",
    "  - You can use the `render` option to save a video instead, find it in your Google Drive\n",
    "- This code was originally taken from [this DQN tutorial](https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c) (with few edits). Study it to answer the following questions.\n",
    "- Learn more about the CartPole [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPLEMENTATION ##\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Do you know what a deque is? https://en.wikipedia.org/wiki/Deque\n",
    "# Though here it's used just to cap its number of elements\n",
    "from collections import deque\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=2000) # after 2000 will delete oldest record(s)\n",
    "\n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.01\n",
    "        self.tau = .125\n",
    "        self.batch_size = 64 # try different batch sizes\n",
    "        \n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(48, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state).flatten())\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        # This trick records past experience and re-plays for accelerate the training\n",
    "        if len(self.memory) < self.batch_size: return\n",
    "        samples = random.sample(self.memory, self.batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state).flatten())\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    # This part can be confusing: can you explain what is happening line by line?\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN ##\n",
    "\n",
    "# Your Python kernel can crash if you redefine your `env` variable without closing\n",
    "# the gym environment properly. But also crashes if you call a method on `env` if\n",
    "# it has not been defined yet. The `try except` construct in Python allows you to \n",
    "# literally try to execute some code while catching errors and exceptions if\n",
    "# something goes wrong. Search online for what a NameError is!\n",
    "try: env.close()\n",
    "except NameError: pass # `pass` literally does nothing\n",
    "\n",
    "env     = gym.make(\"CartPole-v1\")\n",
    "gamma   = 0.9\n",
    "# Sometimes you will find code that uses (1-epsilon) for epsilon\n",
    "# Other times you will find a _decaying_ epsilon, lowering over time\n",
    "epsilon = .5 # it was .9 originally, play with it and understand its role\n",
    "\n",
    "ntrials   = 5 # I can see the beginning of learning with 20, feel free to raise this\n",
    "max_nstep = 200 # This environment is capped at 200 anyway, but you can try shorter\n",
    "\n",
    "dqn_agent = DQN(env=env)\n",
    "for trial in range(ntrials):\n",
    "    print(f\"Trial {trial+1}:\")\n",
    "    cur_state, _ = env.reset() # never forget to reset the env\n",
    "    cur_state = cur_state.reshape(1, -1)\n",
    "    tot_reward = 0\n",
    "    for step in range(max_nstep):\n",
    "        # print('.', end='', flush=True)\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        print(action, end='', flush=True)\n",
    "        \n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        if done: reward = -100 # It's much easier to learn if you punish failing\n",
    "        tot_reward += reward\n",
    "\n",
    "        new_state = new_state.reshape(1,-1)\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "        dqn_agent.replay()       # Internally iterates default (prediction) model\n",
    "        dqn_agent.target_train() # What does this do?\n",
    "        \n",
    "        if done: break\n",
    "    print(f\" Reward: {tot_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Here is how to save trained Keras models\n",
    "dqn_agent.model.save(\"trained.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 **[3pt]** There are three bugs in the implementation. Fix them until it runs successfully.\n",
    "\n",
    "- One is in `## IMPLEMENTATION ##`, easy to find because it stops it from running.\n",
    "- One is just harder, messes with the policy. It's also in `## IMPLEMENTATION ##` but there's a hint in `## MAIN ##`.\n",
    "- One is a missing line in `## MAIN ##`: something there makes no sense.\n",
    "- Of course you can just look at the original implementation. But you will learn less and will not be able do it at the exam, so give it a fair try first, because it is a fair exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 **[1pt]** Compare the shape of the model used with that of an autoencoder. Give one reason as to why they do or do not look alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 **[1pt]** How many inputs and outputs neurons does the model have? Write the code that answers the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 **[3pt]** Write Python code to (i) create a new DQN agent, (ii) load a trained model into the agent's `model`, (iii) change the agent's epsilon-related parameters to enforce a greedy policy, and (iv) runs the greedy policy rendering each frame.\n",
    "\n",
    "- Remember to instantiate the environment first, and to close it at the end, or you may have problems.\n",
    "- Loading the model is easy -- if you saved it to a file in the `main` cell.\n",
    "- To enforce the greedy policy you need to set three agent variables, to sensible values.\n",
    "- You saw a similar evaluation loop when implementing Q-Learning. Just double check the differences with the DQN implementation to make sure it runs. You need to use `env.render()` here (if you're local; if you work on Colab just write a comment about it or save a video).\n",
    "- You can use `time.sleep()` to slow down the loop if the rendering is too fast. Remember also that the env's rendering window will close when you close the environment, so a pause there can also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Solve another of the [OpenAI Gym environments](https://gym.openai.com/envs/#classic_control) by copying and modifying the above DQN implementation. This is very useful to learn to use different environments, and to see first-hand the limits of DRL. Can you imagine why the Mountain Car is hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Augment the DQN implementation to track episode size and cumulative reward and plot them with lineplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning is both the name of the learning paradigm and of the framework classically used to address such problems.\n",
    "- The amount of research currently dedicated to solve RL problems by improving both DL and the RL framework is staggering. Results still keep coming, but at a much slower pace than in purer SL applications (in front of orders of magnitude more investment).\n",
    "- Next week we will explore the limitations of the RL framework, and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
